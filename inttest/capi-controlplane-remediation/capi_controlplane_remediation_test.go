/*
Copyright 2024.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package capicontolplaneremediation

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"strconv"
	"strings"
	"testing"
	"time"

	"github.com/k0smotron/k0smotron/inttest/util"
	"github.com/stretchr/testify/suite"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
)

type CAPIControlplaneHealthCheckRemediation struct {
	suite.Suite
	client           *kubernetes.Clientset
	restConfig       *rest.Config
	clusterYamlsPath string
	ctx              context.Context
}

func TestCAPIControlplaneHealthCheckRemediation(t *testing.T) {
	s := CAPIControlplaneHealthCheckRemediation{}
	suite.Run(t, &s)
}

func (s *CAPIControlplaneHealthCheckRemediation) SetupSuite() {
	kubeConfigPath := os.Getenv("KUBECONFIG")
	s.Require().NotEmpty(kubeConfigPath, "KUBECONFIG env var must be set and point to kind cluster")
	// Get kube client from kubeconfig
	restCfg, err := clientcmd.BuildConfigFromFlags("", kubeConfigPath)
	s.Require().NoError(err)
	s.Require().NotNil(restCfg)
	s.restConfig = restCfg

	// Get kube client from kubeconfig
	kubeClient, err := kubernetes.NewForConfig(restCfg)
	s.Require().NoError(err)
	s.Require().NotNil(kubeClient)
	s.client = kubeClient

	tmpDir := s.T().TempDir()
	s.clusterYamlsPath = tmpDir + "/cluster.yaml"
	s.Require().NoError(os.WriteFile(s.clusterYamlsPath, []byte(testEnvironmentDeclaration), 0644))

	s.ctx, _ = util.NewSuiteContext(s.T())
}

func (s *CAPIControlplaneHealthCheckRemediation) TestCAPIControlPlaneDockerDownScaling() {

	// Apply the child cluster objects
	s.applyClusterObjects()
	defer func() {
		keep := os.Getenv("KEEP_AFTER_TEST")
		if keep == "true" {
			return
		}
		if keep == "on-failure" && s.T().Failed() {
			return
		}
		s.T().Log("Deleting cluster objects")
		s.deleteCluster()
	}()

	var localPort int
	err := wait.PollUntilContextCancel(s.ctx, 1*time.Second, true, func(ctx context.Context) (bool, error) {
		localPort, _ = getLBPort("docker-test-cluster-lb")
		return localPort > 0, nil
	})
	s.Require().NoError(err)

	s.T().Log("waiting to see admin kubeconfig secret")
	kmcKC, err := util.GetKMCClientSet(s.ctx, s.client, "docker-test-cluster", "default", localPort)
	s.Require().NoError(err)

	// nolint:staticcheck
	err = wait.PollImmediateUntilWithContext(s.ctx, 1*time.Second, func(ctx context.Context) (bool, error) {
		b, _ := s.client.RESTClient().
			Get().
			AbsPath("/healthz").
			DoRaw(context.Background())

		return string(b) == "ok", nil
	})
	s.Require().NoError(err)

	s.T().Log("waiting for control-plane nodes to be ready")
	for i := 0; i < 3; i++ {
		// nolint:staticcheck
		err = wait.PollImmediateUntilWithContext(s.ctx, 1*time.Second, func(ctx context.Context) (bool, error) {
			nodeName := fmt.Sprintf("docker-test-cluster-docker-test-%d", i)
			output, err := exec.Command("docker", "exec", nodeName, "k0s", "status").Output()
			if err != nil {
				return false, nil
			}

			return strings.Contains(string(output), "Version:"), nil
		})
		s.Require().NoError(err)
	}

	s.T().Log("waiting for worker-node to be ready")
	s.Require().NoError(util.WaitForNodeReadyStatus(s.ctx, kmcKC, "docker-test-cluster-docker-test-worker-0", corev1.ConditionTrue))

	s.T().Log("forcing a machine healtch check and wait for recreation")
	// to force MachineHealthCheck controller takes action, we use 'cluster.x-k8s.io/remediate-machine' annotation on the machine
	s.forceMHC()

	time.Sleep(time.Minute)
	s.T().Log("waiting for expected control-plane replicas without old annotation related to forcing remediation")
	for i := 0; i < 3; i++ {
		err = wait.PollUntilContextCancel(s.ctx, 50*time.Millisecond, true, func(ctx context.Context) (bool, error) {
			nodeName := fmt.Sprintf("docker-test-cluster-docker-test-%d", i)
			output, err := exec.Command("docker", "exec", nodeName, "k0s", "status").Output()
			if err != nil {
				return false, nil
			}

			containsVersion := strings.Contains(string(output), "Version:")

			// condition to check that new machine is generated by checking metadata.deletiontimestamp is zero.
			if i == 0 {
				return containsVersion && s.isNewMachine(), nil
			}

			return containsVersion, nil
		})
		s.Require().NoError(err)
	}
}

func (s *CAPIControlplaneHealthCheckRemediation) applyClusterObjects() {
	// Exec via kubectl
	out, err := exec.Command("kubectl", "apply", "-f", s.clusterYamlsPath).CombinedOutput()
	s.Require().NoError(err, "failed to apply cluster objects: %s", string(out))
}

func (s *CAPIControlplaneHealthCheckRemediation) deleteCluster() {
	// Exec via kubectl
	out, err := exec.Command("kubectl", "delete", "-f", s.clusterYamlsPath).CombinedOutput()
	s.Require().NoError(err, "failed to delete cluster objects: %s", string(out))
}

func (s *CAPIControlplaneHealthCheckRemediation) forceMHC() {
	// Exec via kubectl
	out, err := exec.Command(
		"kubectl",
		"patch",
		"machine",
		"docker-test-cluster-docker-test-0",
		"--type=merge",
		"-p",
		`{"metadata": {"annotations": {"cluster.x-k8s.io/remediate-machine": ""}}}`,
	).CombinedOutput()
	s.Require().NoError(err, "failed to patch machine to force a healthcheck: %s", string(out))
}

func (s *CAPIControlplaneHealthCheckRemediation) isNewMachine() bool {
	// Exec via kubectl
	out, err := exec.Command(
		"sh",
		"-c",
		`kubectl get machine docker-test-cluster-docker-test-0 -o json | jq '.metadata.annotations | has("cluster.x-k8s.io/remediate-machine")'`,
	).CombinedOutput()
	s.Require().NoError(err, "failed to check if annotation present in machine: %s", string(out))
	return strings.Replace(string(out), "\n", "", 1) != "true"
}

func getLBPort(name string) (int, error) {
	b, err := exec.Command("docker", "inspect", name, "--format", "{{json .NetworkSettings.Ports}}").Output()
	if err != nil {
		return 0, fmt.Errorf("failed to get inspect info from container %s: %w", name, err)
	}

	var ports map[string][]map[string]string
	err = json.Unmarshal(b, &ports)
	if err != nil {
		return 0, fmt.Errorf("failed to unmarshal inspect info from container %s: %w", name, err)
	}

	return strconv.Atoi(ports["6443/tcp"][0]["HostPort"])
}

var testEnvironmentDeclaration = `
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: docker-test-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
      - 10.128.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: K0sControlPlane
    name: docker-test-cluster-docker-test
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DockerCluster
    name: docker-test
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: docker-test-cp-template
  namespace: default
spec:
  template:
    spec:
      customImage: kindest/node:v1.31.0
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: K0sControlPlane
metadata:
  name: docker-test-cluster-docker-test
spec:
  replicas: 3
  version: v1.31.2+k0s.0
  k0sConfigSpec:
    args:
    - --enable-worker
    k0s:
      apiVersion: k0s.k0sproject.io/v1beta1
      kind: ClusterConfig
      metadata:
        name: k0s
      spec:
        api:
          extraArgs:
            anonymous-auth: "true"
        telemetry:
          enabled: false
        network:
          controlPlaneLoadBalancing:
            enabled: false 
    files:
    - path: /tmp/test-file-secret
      contentFrom: 
        secretRef: 
          name: test-file-secret
          key: value
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DockerMachineTemplate
      name: docker-test-cp-template
      namespace: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerCluster
metadata:
  name: docker-test
  namespace: default
spec:
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Machine
metadata:
  name:  docker-test-cluster-docker-test-worker-0
  namespace: default
spec:
  version: v1.31.2
  clusterName: docker-test-cluster
  bootstrap:
    configRef:
      apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
      kind: K0sWorkerConfig
      name: docker-test-worker-0
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DockerMachine
    name: docker-test-cluster-docker-test-worker-0
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: K0sWorkerConfig
metadata:
  name: docker-test-worker-0
  namespace: default
spec:
  # version is deliberately different to be able to verify we actually pick it up :)
  version: v1.31.2+k0s.0
  args:
    - --labels=k0sproject.io/foo=bar
  preStartCommands:
    - echo -n "pre-start" > /tmp/pre-start
  postStartCommands:
    - echo -n "post-start" > /tmp/post-start
  files:
    - path: /tmp/test-file
      content: test-file
    - path: /tmp/test-file-secret
      contentFrom: 
        secretRef: 
          name: test-file-secret
          key: value
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachine
metadata:
  name: docker-test-cluster-docker-test-worker-0
  namespace: default
spec:
  customImage: kindest/node:v1.31.0
---
apiVersion: v1
kind: Secret
metadata:
  name: test-file-secret
  namespace: default
type: Opaque
data:
  value: dGVzdA==
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: docker-test-cluster-kcp-unhealthy-5m
spec:
  clusterName: docker-test-cluster
  maxUnhealthy: 100%
  selector:
    matchLabels:
      cluster.x-k8s.io/control-plane: "true"
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 500s
    - type: Ready
      status: "False"
      timeout: 500s
`
